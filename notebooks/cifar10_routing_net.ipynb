{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks and Deep Learning coursework\n",
        "\n",
        "---\n",
        "\n",
        "# Student ID:210899247\n"
      ],
      "metadata": {
        "id": "GCyRft7gCV_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Drive Setup**\n",
        "\n",
        "---\n",
        "\n",
        "Following the lecture explanations, I mounted my Google Drive so I could use external file \"my_utils.py\" provided on qmplus inside the Colab environment. In addition, to find the file, I have added path to the directory where its stored using sys.path.append."
      ],
      "metadata": {
        "id": "E67nyIoMCrpG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODXo4XLMCVUR",
        "outputId": "23442e3d-a68e-40a7-f22a-f7092ed93922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# === Google Drive Setup ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "# Mounting Google Drive so I can access the file stored there.\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')\n",
        "# Adding the path where my my_utils.py is saved so I can import it like a normal Python module."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary packages**\n",
        "\n",
        "---\n",
        "\n",
        "Here I am setting up all imports I would need for this coursework. This includes everything, from PyTorch and torchvision to helper libraries like tqdm for progress bars and matplotlib for visualisations of the accuracy results. I am also importing my_utils to use it within the assignment."
      ],
      "metadata": {
        "id": "UpwBf3dUE-Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "\n",
        "# The custom utility file from qmplus with helper functions I used in different parts of the training loop\n",
        "import my_utils as mu\n",
        "\n",
        "# Core PyTorch library\n",
        "import torch\n",
        "\n",
        "# Needed for building neural networks\n",
        "from torch import nn\n",
        "\n",
        "# Useful PyTorch functions for things like activations and loss computations\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Torchvision gives access to standard datasets like CIFAR-10 and includes transforms\n",
        "import torchvision\n",
        "\n",
        "# Contains predefined transforms for image preprocessing and augmentation\n",
        "from torchvision import transforms\n",
        "\n",
        "# For loading data in batches and shuffling\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# For plotting accuracy/loss graphs at the end\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NumPy is helpful for generating random values (used in mixup, for instance)\n",
        "import numpy as np\n",
        "\n",
        "# Adds a progress bar to loops — super handy to monitor training\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Y9p07hUHE9pf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mixup functions**\n",
        "\n",
        "---\n",
        "\n",
        "In this part I implement mixup which is a data augmentation technique that blends two images and their labels. I only apply it during first few epoches of training to help the model generalise better and reduce overfitting. I have also used a helper function mixup_criterion here that computes the loss using mixedup targets"
      ],
      "metadata": {
        "id": "baKoKXJtJATA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, alpha=1.0):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "\n",
        "    # If alpha is positive, we sample lambda from the beta distribution. This controls how much mixing happens.\n",
        "    # The closer lambda is to 1 or 0, the more like one sample it will be.\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1  # No mixing at all if alpha is zero\n",
        "\n",
        "    batch_size = x.size()[0]  # Number of samples in the batch\n",
        "    index = torch.randperm(batch_size).to(x.device)  # Random permutation of the batch indices, used to shuffle\n",
        "\n",
        "    # Mix the images: a weighted sum of the original batch and a randomly shuffled batch\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "\n",
        "    # This allows us to keep track of the original labels and the shuffled labels\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam  # Returns everything needed for computing the mixed-up loss\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    # Loss is also blended: proportionally apply the loss to both original and shuffled labels\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
      ],
      "metadata": {
        "id": "pFntHqaCJAmA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Device Setup**\n",
        "\n",
        "---\n",
        "\n",
        "This part sets up the computation device to GPU to train faster. Since I am training on CIFAR-10, which is quite slow on CPU"
      ],
      "metadata": {
        "id": "XyFBS5ecKX5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Device Setup ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")  # This helps verify whether we're using the GPU or CPU\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZdqzgOAKYie",
        "outputId": "63d1d5bc-12f1-43ef-da50-7533fa2003f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Transformations**\n",
        "\n",
        "---\n",
        "\n",
        "Here I am setting up data augmentation and normalisation for CIFAR-10 dataset. I have also applied few common data augmentation techniques like random horizontal flipping, random cropping with padding, and color jitter to add slight variations to brightness, contrast, and saturation. This helps the model generalise better by not overfitting to fixed patterns in the training data.\n",
        "Lastly, these transformations are all composed and will be applied to both training and validation(testing) datasets"
      ],
      "metadata": {
        "id": "muQibEnzLgpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Data Transformations ===\n",
        "# Normalize using CIFAR-10 statistics: mean/std per channel\n",
        "cifar_mean = (0.4915, 0.4823, 0.4466)  # channel-wise means for RGB\n",
        "cifar_std = (0.2024, 0.1995, 0.2011)   # channel-wise std devs for RGB\n",
        "\n",
        "transform_config = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # randomly flip the image horizontally\n",
        "    transforms.RandomCrop(32, padding=4),  # randomly crop image with padding to preserve structure\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # add color variation\n",
        "    transforms.ToTensor(),  # convert image from PIL to tensor\n",
        "    transforms.Normalize(cifar_mean, cifar_std)  # normalize with CIFAR-10 stats\n",
        "])\n"
      ],
      "metadata": {
        "id": "nRiXP7NkLhGS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Download and Preprocessing**\n",
        "\n",
        "---\n",
        "\n",
        "Here I am loading CIFAR-10 dataset. When I load the datasets, I pass in the transformation pipeline I defined earlier so that all the augmentation and normalisation is automatically applied to each image when it is loaded. In addition, I explicitly set download=True to download the dataset and set train=True for training set and train=Falase for the test (validation) set"
      ],
      "metadata": {
        "id": "NJzfqGwTMxRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Dataset Download and Preprocessing ===\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform_config, download=True)\n",
        "val_data = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform_config, download=True)\n"
      ],
      "metadata": {
        "id": "BCdCMDwvMxz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0894dd68-228c-410a-838e-df265d02bef2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:10<00:00, 16.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader Setup**\n",
        "\n",
        "---\n",
        "\n",
        "In this part I set up the data loaders for both training and validation dataset. I used a batch size of 256, which works well with my GPU and provides a good balance between performance and memory usage."
      ],
      "metadata": {
        "id": "Njz13aYhOr8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Data Loaders ===\n",
        "\n",
        "# Batch size used for training and validation\n",
        "loader_batch_size = 256\n",
        "\n",
        "# DataLoader for training data\n",
        "# - shuffle=True randomizes the data order each epoch for better generalization\n",
        "# - num_workers=2 enables two threads to load data in parallel\n",
        "train_loader = DataLoader(train_data, batch_size=loader_batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# DataLoader for validation data\n",
        "# - shuffle=False keeps order consistent for accurate validation\n",
        "val_loader = DataLoader(val_data, batch_size=loader_batch_size, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "jGBkiwjiOsF7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Meta Info**\n",
        "\n",
        "---\n",
        "\n",
        "I am printing the labels here, also I use a function to increase the number of CPU threads Pytorch can use. This makes the data loading process more effective"
      ],
      "metadata": {
        "id": "I8TB_mOiPT92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Meta Info ===\n",
        "class_labels = train_data.classes  # Store CIFAR-10 class names for later reference\n",
        "print(\"CIFAR-10 Classes:\", class_labels)  # Print them to make sure they're correct\n",
        "\n",
        "torch.set_num_threads(8)  # Use more CPU threads to speed up data loading and preprocessing\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMWglvAHPUGs",
        "outputId": "5935146f-d28b-4492-f0d9-24da1bfdd864"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model architecture - Stem Block**\n",
        "\n",
        "---\n",
        "\n",
        "Here I am building a StemBlock which is the initial layer of my CNN model. It's responsible for receiving the input image and extracting low level features like edges or textures. The block takes the raw image and extracts the initial feature map. This sets up the input so it's in the right form for the deeper layers in the network to process"
      ],
      "metadata": {
        "id": "5-F7zJpkOeAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StemBlock(nn.Module):\n",
        "    # This is the very first part of the model — the \"stem\".\n",
        "    # It takes the raw image input and applies one convolution to start feature extraction.\n",
        "    def __init__(self, in_channels=3, out_channels=48):\n",
        "        # I’m using 3 input channels because CIFAR-10 images are RGB.\n",
        "        # I chose 48 output channels to give a slightly wider feature representation without being too heavy.\n",
        "        super(StemBlock, self).__init__()\n",
        "\n",
        "        # This is the convolution layer.\n",
        "        # I use a 3x3 kernel, stride of 1, and padding of 1 so that the output has the same spatial size as the input.\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # After convolution, I normalize the output using BatchNorm.\n",
        "        # This helps the training converge faster and be more stable.\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Finally, I add a ReLU activation to introduce non-linearity.\n",
        "        # It’s standard practice and helps the network learn more complex patterns.\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # In the forward pass, first I apply the convolution layer\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Then I apply BatchNorm to the conv output\n",
        "        x = self.bn(x)\n",
        "\n",
        "        # Lastly, I activate the result with ReLU\n",
        "        return self.act(x)\n"
      ],
      "metadata": {
        "id": "E71u2uj6OeL1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture - Routing Block - Backbone (B1 to BN)**\n",
        "\n",
        "---\n",
        "\n",
        "This is the backbone of my model, here the learning happens. Each block has an attention mechanism that decides how to combine convolutional experts. I use global average pooling and two linear layers and softmax to generate the attention weights. Then I apply each expert separately and blend their outputs using thsoe weights. Finally, I normalise, drop out, and activate the results."
      ],
      "metadata": {
        "id": "1Nj-dpjEQBQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpertRoutingBlock(nn.Module):\n",
        "    def __init__(self, features, num_experts=2, squeeze_ratio=4):\n",
        "        # This block allows the model to pick between multiple convolution paths.\n",
        "        # 'features' is the number of channels coming in and out of the block.\n",
        "        # 'num_experts' is the number of parallel paths (like 2 specialists).\n",
        "        # 'squeeze_ratio' controls the bottleneck size in the attention MLP.\n",
        "        super(ExpertRoutingBlock, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # Here I create multiple parallel convolution layers (the experts).\n",
        "        # Each one sees the same input, but the model learns to weigh them differently per input.\n",
        "        self.expert_convs = nn.ModuleList([\n",
        "            nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1)\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "        # This is a global average pooling that reduces each feature map to a single value.\n",
        "        # It's used for computing attention weights later.\n",
        "        self.attention_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "\n",
        "        # First FC layer of the attention MLP — reduces dimension (bottleneck).\n",
        "        self.attention_fc1 = nn.Linear(features, features // squeeze_ratio)\n",
        "\n",
        "        # Second FC layer — outputs logits for each expert (routing weights).\n",
        "        self.attention_fc2 = nn.Linear(features // squeeze_ratio, num_experts)\n",
        "\n",
        "        # BatchNorm after combining the expert outputs\n",
        "        self.norm = nn.BatchNorm2d(features)\n",
        "\n",
        "        # ReLU activation after normalization and dropout\n",
        "        self.activate = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get batch size (B), channels (C), height (H), width (W)\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Perform global average pooling, then flatten to (B, C)\n",
        "        pooled = self.attention_pool(x).view(B, C)\n",
        "\n",
        "        # Pass through first FC layer of the MLP and apply ReLU\n",
        "        bottleneck = F.relu(self.attention_fc1(pooled))\n",
        "\n",
        "        # Get the attention weights by softmax over the logits from the second FC\n",
        "        weights = F.softmax(self.attention_fc2(bottleneck), dim=1)  # shape: (B, num_experts)\n",
        "\n",
        "        # Run the input through all expert convs in parallel and stack results\n",
        "        # Output shape: (B, num_experts, C, H, W)\n",
        "        conv_outputs = torch.stack([conv(x) for conv in self.expert_convs], dim=1)\n",
        "\n",
        "        # Reshape attention weights to broadcast across feature maps\n",
        "        weights = weights.view(B, self.num_experts, 1, 1, 1)\n",
        "\n",
        "        # Compute the weighted sum of the expert outputs\n",
        "        blended = (weights * conv_outputs).sum(dim=1)  # shape: (B, C, H, W)\n",
        "\n",
        "        # Normalize the combined result, apply dropout and ReLU\n",
        "        blended = self.norm(blended)\n",
        "        blended = F.dropout(blended, p=0.25, training=self.training)\n",
        "        return self.activate(blended)\n"
      ],
      "metadata": {
        "id": "_EXNrG3HQBZN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture - RoutingNet - Classifier(C)**\n",
        "\n",
        "---\n",
        "\n",
        "This is the classification head of my model. After feature map have gone through backbone, I apply global average pooling to condense the spatial dimensions. Then I apply dropout for reguralisation and a fully connected layer to map to the 10 output classes of CIFAR-10."
      ],
      "metadata": {
        "id": "Q8EoGhvNYgA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RoutingNet(nn.Module):\n",
        "    def __init__(self, num_layers=6, num_paths=2, hidden_channels=256, num_outputs=10):\n",
        "        super(RoutingNet, self).__init__()\n",
        "\n",
        "        # === Stem Block ===\n",
        "        # This is the very first convolutional layer.\n",
        "        # It takes the raw image and extracts some initial low-level features.\n",
        "        self.stem = StemBlock(in_channels=3, out_channels=hidden_channels)\n",
        "\n",
        "        # === Backbone ===\n",
        "        # This is the main part of the network — it's a sequence of ExpertRoutingBlocks.\n",
        "        # Each block contains multiple expert conv layers and attention-based routing.\n",
        "        # I'm stacking `num_layers` of them here.\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            ExpertRoutingBlock(features=hidden_channels, num_experts=num_paths)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # === Classifier Head ===\n",
        "        # This part summarizes the feature map and produces the final class logits.\n",
        "\n",
        "        # This is global average pooling: it collapses the HxW spatial dimensions into a single number per channel.\n",
        "        # That way, I get a fixed-size feature vector of shape (batch_size, channels).\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "\n",
        "        # This dropout layer is used to prevent overfitting before the final layer.\n",
        "        # It's applied after pooling but before classification.\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "\n",
        "        # This is the final linear layer.\n",
        "        # It maps the pooled feature vector (of size `hidden_channels`) to 10 output classes for CIFAR-10.\n",
        "        self.head = nn.Linear(hidden_channels, num_outputs)\n",
        "\n",
        "        # I call a custom weight initialization method to initialize convs and linears properly.\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First, I pass the image through the stem to get low-level features.\n",
        "        x = self.stem(x)\n",
        "\n",
        "        # Then the features go through the full sequence of routing blocks.\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # After the backbone, I pool the features to remove spatial dimensions (global avg pool).\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "\n",
        "        # I apply dropout for regularization.\n",
        "        x = self.drop(x)\n",
        "\n",
        "        # Finally, I pass the feature vector to the linear layer to get the class scores.\n",
        "        return self.head(x)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        # This helper function initializes all conv and linear layers.\n",
        "        # I use Kaiming initialization for ReLU-based models, which helps training stability.\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
        "                if layer.bias is not None:\n",
        "                    nn.init.constant_(layer.bias, 0)\n",
        "# Creating an instance of my custom RoutingNet model\n",
        "# I'm setting:\n",
        "# - num_layers=5: the number of ExpertRoutingBlocks (this forms the backbone of the network)\n",
        "# - num_paths=2: each block has 2 expert convolution branches (like 2 parallel specialists)\n",
        "# - hidden_channels=164: the number of channels for the internal feature maps (kind of like the width of the network)\n",
        "# I send the whole model to the GPU if available (for speed) using `.to(device)`\n",
        "net = RoutingNet(num_layers=5, num_paths=2, hidden_channels=164).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "yTHpezbGYgPJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Configuration: Loss, Optimiser and Scheduler**\n",
        "\n",
        "---\n",
        "\n",
        "Here I define all the core training hyperparameters and components needed to optimise the network: the loss function, optimiser, and the learning rate scheduler. These are all necessary to train the model effectively."
      ],
      "metadata": {
        "id": "zkgvs7hfePRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Loss Function ===\n",
        "# I’m using CrossEntropyLoss since it’s the standard loss function for classification tasks.\n",
        "# I added a small amount of label smoothing (0.005) to improve generalization by preventing the model from being too confident.\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.005)\n",
        "\n",
        "# === Optimizer ===\n",
        "# I’m using SGD (Stochastic Gradient Descent), which is commonly used in computer vision tasks.\n",
        "# I chose a high initial learning rate of 0.19 and added momentum (0.9) for faster convergence.\n",
        "# Weight decay (5e-4) is used for L2 regularization to help prevent overfitting.\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.19, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# This dropout variable was previously used in the classifier but defined here — I left it in case I needed it dynamically.\n",
        "dropout = 0.3\n",
        "\n",
        "# === Learning Rate Scheduler ===\n",
        "# I’m using cosine annealing for learning rate scheduling over 35 epochs.\n",
        "# This gradually reduces the learning rate following a cosine curve down to a minimum of 1e-4.\n",
        "# I do it to improve final convergence and avoid getting stuck in bad minima.\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=35, eta_min=1e-4)\n"
      ],
      "metadata": {
        "id": "gHD63jUaePcX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop and Evaluation**\n",
        "\n",
        "---\n",
        "\n",
        "In this section I have implemented the full training pipeline. This includes the model over multiple epochs, applying mixup data augmentaton for the first 25 epochs, computing training and testing metrics, and stepping the cosine annealing scheduler. I have also added logging for accuracy and loss values across epochs"
      ],
      "metadata": {
        "id": "v8L5_8xPfR01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I decided to train for 35 epochs to give the model enough time to converge.\n",
        "total_epochs = 35\n",
        "\n",
        "# I’ll track the best validation accuracy (useful if I wanted to save best model).\n",
        "best_val_acc = 0.0\n",
        "\n",
        "# These lists will keep track of losses and accuracies for plotting and analysis later.\n",
        "train_losses, val_losses = [], []\n",
        "train_scores, val_scores = [], []\n",
        "\n",
        "# Now starting the main training loop\n",
        "for epoch in range(total_epochs):\n",
        "    # Set the model in training mode so things like dropout work properly\n",
        "    net.train()\n",
        "\n",
        "    # These help track the cumulative loss and correct predictions during the epoch\n",
        "    epoch_loss = 0.0\n",
        "    correct_preds, seen_samples = 0, 0\n",
        "\n",
        "    # Loop through batches in the training set\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n",
        "        # Move data to GPU if available\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Clear any gradients from the last step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # For the first 25 epochs, I apply Mixup regularization\n",
        "        if epoch < 25:\n",
        "            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=0.1)\n",
        "            outputs = net(inputs)\n",
        "            loss = mixup_criterion(loss_fn, outputs, targets_a, targets_b, lam)\n",
        "        else:\n",
        "            # After epoch 25, I go back to using standard training\n",
        "            outputs = net(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backpropagation to compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model weights based on computed gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss (scaled by batch size) and count correct predictions\n",
        "        epoch_loss += loss.item() * inputs.size(0)\n",
        "        preds = outputs.argmax(dim=1)  # Get predicted class\n",
        "        correct_preds += (preds == labels).sum().item()\n",
        "        seen_samples += labels.size(0)\n",
        "\n",
        "    # After the training loop, I step the scheduler to update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Calculate and store average training loss and accuracy for the epoch\n",
        "    avg_train_loss = epoch_loss / seen_samples\n",
        "    train_acc = 100. * correct_preds / seen_samples\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_scores.append(train_acc)\n",
        "\n",
        "    # === VALIDATION ===\n",
        "    # Switch model to evaluation mode — turns off things like dropout\n",
        "    net.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct, val_total = 0, 0\n",
        "\n",
        "    # I don't need gradients when evaluating, so I wrap in torch.no_grad\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            # Accumulate validation loss and accuracy\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    # Calculate average validation loss and accuracy\n",
        "    avg_val_loss = val_loss / val_total\n",
        "    val_acc = 100. * val_correct / val_total\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_scores.append(val_acc)\n",
        "\n",
        "\n",
        "# Update best validation accuracy if this epoch's is higher\n",
        "    if val_acc > best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "\n",
        "\n",
        "    # I print results every epoch so I can track training progress\n",
        "    print(f\"[Epoch {epoch+1}] LR={scheduler.get_last_lr()[0]:.6f} | Training Accuracy={train_acc:.2f}% | Testing Accuracy={val_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nBest Validation Accuracy Achieved: {best_val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5t0V-sGfR9u",
        "outputId": "356d7a88-6516-497c-fd7e-0bc1123660b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/35:   1%|          | 1/196 [01:33<5:04:17, 93.63s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualisation**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here I am plotting the training and testing accuracy and loss curves. This helps to visualise how well the model is learning and generalising. I use it to both understand overfitting."
      ],
      "metadata": {
        "id": "JIS_mvvIgGoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the accuracy curves for both training and validation\n",
        "plt.plot(train_scores, label='Training Accuracy')  # Training accuracy per epoch\n",
        "plt.plot(val_scores, label='Validation Accuracy')  # Validation accuracy per epoch\n",
        "plt.title(\"Accuracy over Epochs\")  # Title of the plot\n",
        "plt.xlabel(\"Epoch\")  # X-axis label\n",
        "plt.ylabel(\"Accuracy (%)\")  # Y-axis label\n",
        "plt.legend()  # Add a legend to differentiate lines\n",
        "plt.grid(True)  # Add a grid for better readability\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Plotting the loss curves for both training and validation\n",
        "plt.plot(train_losses, label='Training Loss')  # Training loss per epoch\n",
        "plt.plot(val_losses, label='Validation Loss')  # Validation loss per epoch\n",
        "plt.title(\"Loss over Epochs\")  # Title of the plot\n",
        "plt.xlabel(\"Epoch\")  # X-axis label\n",
        "plt.ylabel(\"Loss\")  # Y-axis label\n",
        "plt.legend()  # Add a legend to differentiate lines\n",
        "plt.grid(True)  # Add a grid for better readability\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Accuracy gap = Train Acc - Val Acc\n",
        "accuracy_gap = [train - val for train, val in zip(train_scores, val_scores)]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(accuracy_gap, label='Accuracy Gap (Train - Val)')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.title(\"Accuracy Gap Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Gap (%)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(val_scores[-20:], marker='o', label='Val Acc (Last 20 Epochs)')\n",
        "plt.title(\"Validation Accuracy - Last 20 Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xticks(ticks=range(20), labels=range(len(val_scores)-19, len(val_scores)+1))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "def moving_average(data, window=5):\n",
        "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
        "\n",
        "plt.plot(moving_average(train_scores), label='Train Accuracy (Smoothed)')\n",
        "plt.plot(moving_average(val_scores), label='Val Accuracy (Smoothed)')\n",
        "plt.title(\"Smoothed Accuracy Curves\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eARuhsPLgGzC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}